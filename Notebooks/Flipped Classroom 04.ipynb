{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flipped Classroom 04: Training a Twin Model on MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import datetime\n",
    "\n",
    "# magic line only needed in jupyter notebooks!\n",
    "%load_ext tensorboard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataset\n",
    "\n",
    "**Task: take two random mnist digit 28x28x1 images and compute the equality of the numbers that they display.**\n",
    "\n",
    "\n",
    "Input to the model will be: batch-size times two randomly chosen mnist digits, flattened to vectors.\n",
    "\n",
    "Output of the model (its targets) should be: for each element in the batch, 1 if the two mnist images have the same label, and 0 otherwise.\n",
    "\n",
    "*Notice how this will lead to an imbalanced classification dataset. We can address this by setting a sample_weight argument in the loss calculation, weighting the contribution to gradients stronger for the rare class than for the common class. We have similar issues with class imbalance in many applied fields (medicine, finance, fraud-detection etc.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get mnist from tensorflow_datasets\n",
    "mnist = tfds.load(\"mnist\", split =[\"train\",\"test\"], as_supervised=True)\n",
    "train_ds = mnist[0]\n",
    "val_ds = mnist[1]\n",
    "\n",
    "# 2. write function to create the dataset that we want\n",
    "def preprocess(data, batch_size):\n",
    "    # image should be float\n",
    "    data = data.map(lambda x, t: (tf.cast(x, float), t))\n",
    "    # image should be flattened\n",
    "    data = data.map(lambda x, t: (tf.reshape(x, (-1,)), t))\n",
    "    # image vector will here have values between -1 and 1\n",
    "    data = data.map(lambda x,t: ((x/128.)-1., t))\n",
    "    # we want to have two mnist images in each example\n",
    "    # this leads to a single example being ((x1,y1),(x2,y2))\n",
    "    zipped_ds = tf.data.Dataset.zip((data.shuffle(2000), \n",
    "                                     data.shuffle(2000)))\n",
    "    # map ((x1,y1),(x2,y2)) to (x1,x2, y1==y2*) *boolean\n",
    "    zipped_ds = zipped_ds.map(lambda x1, x2: (x1[0], x2[0], x1[1]==x2[1]))\n",
    "    # transform boolean target to int\n",
    "    zipped_ds = zipped_ds.map(lambda x1, x2, t: (x1,x2, tf.cast(t, tf.int32)))\n",
    "    # batch the dataset\n",
    "    zipped_ds = zipped_ds.batch(batch_size)\n",
    "    # prefetch\n",
    "    zipped_ds = zipped_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return zipped_ds\n",
    "\n",
    "train_ds = preprocess(train_ds, batch_size=32) #train_ds.apply(preprocess)\n",
    "val_ds = preprocess(val_ds, batch_size=32) #val_ds.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 784) (32, 784) (32,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-25 12:31:05.975626: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-11-25 12:31:05.978701: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# check the contents of the dataset\n",
    "for img1, img2, label in train_ds:\n",
    "    print(img1.shape, img2.shape, label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model for the task\n",
    "\n",
    "- The two images should be processed using the same parameters (sometimes called a siamese or twin model)\n",
    "\n",
    "- What do we need?\n",
    "\n",
    "1. Subclass from **tf.keras.Model**\n",
    "\n",
    "2. Implement **constructor method** that contains \n",
    "    - a) metrics objects to keep track (tf.keras.metrics)\n",
    "    - b) optimizer\n",
    "    - c) loss function\n",
    "    - d) the layers that are used for prediction, incl. output layer\n",
    "3. A **call method** that applies the computations (makes use of the model's layers)\n",
    "    - has a training flag argument to allow for different training and inference behavior\n",
    "    - uses the same layers for both images and then e.g. concatenates the representations before feeding them to an output layer.\n",
    "    \n",
    "4. A **metrics property** that returns a list of all metrics objects in the model\n",
    "\n",
    "5. A **reset_metrics** method that iterates through the metrics in the metrics property and resets their state.\n",
    "    - This is called before after each epoch and between training and validation during an epoch (to not mix train score with val score)\n",
    "    \n",
    "6. A **train_step** method that takes a data tuple as input, containing one batch of e.g. inputs, targets. In this case it is (inputs_1, inputs_2, target)\n",
    "    - compute prediction and loss within a gradient tape context\n",
    "    - get gradients w.r.t. trainable variables from the tape\n",
    "    - pass the gradients and the trainable variables to the optimizer\n",
    "    - update the metrics' states using the loss, the prediction and the target\n",
    "    - return a dictionary containing each metric's name as a key and the corresponding result state as its value. This allows to easily print, and it is needed for the more high level compile and fit methods.\n",
    "    \n",
    "    \n",
    "7. A **test_step** method that does the same as the train_step but without gradient calculations or optimization. \n",
    "    - The training flag in the model's call method should be set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwinMNISTModel(tf.keras.Model):\n",
    "\n",
    "    # 1. constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # inherit functionality from parent class\n",
    "\n",
    "        # optimizer, loss function and metrics\n",
    "        self.metrics_list = [tf.keras.metrics.BinaryAccuracy(),\n",
    "                             tf.keras.metrics.Mean(name=\"loss\")]\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        self.loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "        # layers to encode the images (both layers used for both images)\n",
    "        self.dense1 = tf.keras.layers.Dense(128, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(128, activation=tf.nn.relu)\n",
    "        \n",
    "        self.dense3 = tf.keras.layers.Dense(128, activation=tf.nn.relu)\n",
    "        \n",
    "        self.out_layer = tf.keras.layers.Dense(1,activation=tf.nn.sigmoid)\n",
    "        \n",
    "    # 2. call method (forward computation)\n",
    "    def call(self, images, training=False):\n",
    "        img1, img2 = images\n",
    "        \n",
    "        img1_x = self.dense1(img1)\n",
    "        img1_x = self.dense2(img1_x)\n",
    "        \n",
    "        img2_x = self.dense1(img2)\n",
    "        img2_x = self.dense2(img2_x)\n",
    "        \n",
    "        combined_x = tf.concat([img1_x, img2_x ], axis=1)\n",
    "        combined_x = self.dense3(combined_x)\n",
    "        return self.out_layer(combined_x)\n",
    "\n",
    "    # 3. metrics property\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "        # return a list with all metrics in the model\n",
    "\n",
    "    # 4. reset all metrics objects\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "\n",
    "    # 5. train step method\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        img1, img2, label = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self((img1, img2), training=True)\n",
    "            loss = self.loss_function(label, output)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # update the state of the metrics according to loss\n",
    "        self.metrics[0].update_state(label, output)\n",
    "        self.metrics[1].update_state(loss)\n",
    "        \n",
    "        # return a dictionary with metric names as keys and metric results as values\n",
    "        return {m.name : m.result() for m in self.metrics}\n",
    "\n",
    "    # 6. test_step method\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        img1, img2, label = data\n",
    "        # same as train step (without parameter updates)\n",
    "        output = self((img1, img2), training=False)\n",
    "        loss = self.loss_function(label, output)\n",
    "        self.metrics[0].update_state(label, output)\n",
    "        self.metrics[1].update_state(loss)\n",
    "        \n",
    "        return {m.name : m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a summary writer to log data\n",
    "\n",
    "- use tf.summary.create_file_writer(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_writers(config_name):\n",
    "    \n",
    "    # Define where to save the logs\n",
    "    # along with this, you may want to save a config file with the same name so you know what the hyperparameters were used\n",
    "    # alternatively make a copy of the code that is used for later reference\n",
    "    \n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "    val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
    "\n",
    "    # log writer for training metrics\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "    # log writer for validation metrics\n",
    "    val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
    "    \n",
    "    return train_summary_writer, val_summary_writer\n",
    "\n",
    "train_summary_writer, val_summary_writer = create_summary_writers(config_name=\"RUN1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a training loop function\n",
    "\n",
    "Arguments: \n",
    " - the model to train, \n",
    " - the data to train on, \n",
    " - the data to test on, \n",
    " - what the first epoch is,\n",
    " - how many epochs to train, \n",
    " - the train summary writer object to use for logging\n",
    " - the validation summary writer object to use for logging\n",
    " - a path to save trained model weights to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def training_loop(model, train_ds, val_ds, start_epoch,\n",
    "                  epochs, train_summary_writer, \n",
    "                  val_summary_writer, save_path):\n",
    "\n",
    "    # 1. iterate over epochs\n",
    "    for e in range(start_epoch, epochs):\n",
    "\n",
    "        # 2. train steps on all batches in the training data\n",
    "        for data in tqdm.tqdm(train_ds, position=0, leave=True):\n",
    "            metrics = model.train_step(data)\n",
    "\n",
    "        # 3. log and print training metrics\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            # for scalar metrics:\n",
    "            for metric in model.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=e)\n",
    "            # alternatively, log metrics individually (allows for non-scalar metrics such as tf.keras.metrics.MeanTensor)\n",
    "            # e.g. tf.summary.image(name=\"mean_activation_layer3\", data = metrics[\"mean_activation_layer3\"],step=e)\n",
    "        \n",
    "        #print the metrics\n",
    "        print([f\"{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "        \n",
    "        # 4. reset metric objects\n",
    "        model.reset_metrics()\n",
    "\n",
    "\n",
    "        # 5. evaluate on validation data\n",
    "        for data in val_ds:\n",
    "            metrics = model.test_step(data)\n",
    "        \n",
    "\n",
    "        # 6. log validation metrics\n",
    "\n",
    "        with val_summary_writer.as_default():\n",
    "            # for scalar metrics:\n",
    "            for metric in model.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=e)\n",
    "            # alternatively, log metrics individually (allows for non-scalar metrics such as tf.keras.metrics.MeanTensor)\n",
    "            # e.g. tf.summary.image(name=\"mean_activation_layer3\", data = metrics[\"mean_activation_layer3\"],step=e)\n",
    "            \n",
    "        print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "        # 7. reset metric objects\n",
    "        model.reset_metrics()\n",
    "        \n",
    "    # 8. save model weights if save_path is given\n",
    "    if save_path:\n",
    "        model.save_weights(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the training loop function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bc653777658af544\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bc653777658af544\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# open the tensorboard logs\n",
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:05<00:00, 354.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9060333371162415', 'loss: 0.26210615038871765']\n",
      "['val_binary_accuracy: 0.9147999882698059', 'val_loss: 0.21181651949882507']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:03<00:00, 476.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9337499737739563', 'loss: 0.16407448053359985']\n",
      "['val_binary_accuracy: 0.9488000273704529', 'val_loss: 0.14193446934223175']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:03<00:00, 481.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9487500190734863', 'loss: 0.13859908282756805']\n",
      "['val_binary_accuracy: 0.9605000019073486', 'val_loss: 0.11177009344100952']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:03<00:00, 488.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.960099995136261', 'loss: 0.11381417512893677']\n",
      "['val_binary_accuracy: 0.9620000123977661', 'val_loss: 0.10693037509918213']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:03<00:00, 480.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9645333290100098', 'loss: 0.10216216742992401']\n",
      "['val_binary_accuracy: 0.9664999842643738', 'val_loss: 0.09265995025634766']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:03<00:00, 472.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9659166932106018', 'loss: 0.09643787145614624']\n",
      "['val_binary_accuracy: 0.9732999801635742', 'val_loss: 0.07639753073453903']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:05<00:00, 355.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9688000082969666', 'loss: 0.090479277074337']\n",
      "['val_binary_accuracy: 0.9628000259399414', 'val_loss: 0.10456037521362305']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:05<00:00, 357.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9696666598320007', 'loss: 0.08797740936279297']\n",
      "['val_binary_accuracy: 0.9688000082969666', 'val_loss: 0.08604127168655396']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:05<00:00, 348.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9714499711990356', 'loss: 0.0812680646777153']\n",
      "['val_binary_accuracy: 0.9722999930381775', 'val_loss: 0.07755609601736069']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:05<00:00, 362.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9710833430290222', 'loss: 0.08124373108148575']\n",
      "['val_binary_accuracy: 0.9736999869346619', 'val_loss: 0.07744208723306656']\n"
     ]
    }
   ],
   "source": [
    "# 1. instantiate model\n",
    "model = TwinMNISTModel()\n",
    "\n",
    "# 2. choose a path to save the weights\n",
    "save_path = \"trained_model_RUN1\"\n",
    "\n",
    "# 2. pass arguments to training loop function\n",
    "\n",
    "training_loop(model=model,\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    start_epoch=0,\n",
    "    epochs=10,\n",
    "    train_summary_writer=train_summary_writer,\n",
    "    val_summary_writer=val_summary_writer,\n",
    "    save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model:\n",
    "fresh_model = TwinMNISTModel()\n",
    "\n",
    "# build the model's parameters by calling it on input\n",
    "for img1,img2,label in train_ds:\n",
    "    fresh_model((img1,img2));\n",
    "    break\n",
    "\n",
    "# load the saved weights\n",
    "model = fresh_model.load_weights(save_path)\n",
    "\n",
    "# we could now continue training (will use a fresh optimizer, without the old optimizer state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (OPTIONAL) Bonus: Sample weights\n",
    "\n",
    "Below you can find the same model where the loss is weighted by the probability of both classes occuring. The class \"images are showing the same number\" has a probability of 0.1, while the case in which the numbers are not the same has a probability of 0.9. We can weight the loss contribution (and hence the gradient contribution) by the probability of not occuring. For every element in a batch, we weight its contribution to the loss by 0.9 if this element is showing label 1 (images are the same), and by 0.1 if the label is 0. Since this class is much more frequent, but we want equal contribution to gradients across the classes.\n",
    "\n",
    "To get these sample weights, we write a new method \"get_sample_weights\" and use tf.where to fill the sample weights tensor according to a boolean tensor obtained from the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwinMNISTModelSW(tf.keras.Model):\n",
    "\n",
    "    # 1. constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # inherit functionality from parent class\n",
    "\n",
    "        # optimizer, loss function and metrics\n",
    "        self.metrics_list = [tf.keras.metrics.BinaryAccuracy(),\n",
    "                             tf.keras.metrics.Mean(name=\"loss\")]\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        self.loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "        # layers to encode the images (both layers used for both images)\n",
    "        self.dense1 = tf.keras.layers.Dense(128, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(128, activation=tf.nn.relu)\n",
    "        \n",
    "        self.dense3 = tf.keras.layers.Dense(128, activation=tf.nn.relu)\n",
    "        \n",
    "        self.out_layer = tf.keras.layers.Dense(1,activation=tf.nn.sigmoid)\n",
    "        \n",
    "    # 2. call method (forward computation)\n",
    "    def call(self, images, training=False):\n",
    "        img1, img2 = images\n",
    "        \n",
    "        img1_x = self.dense1(img1)\n",
    "        img1_x = self.dense2(img1_x)\n",
    "        \n",
    "        img2_x = self.dense1(img2)\n",
    "        img2_x = self.dense2(img2_x)\n",
    "        \n",
    "        combined_x = tf.concat([img1_x, img2_x ], axis=1)\n",
    "        combined_x = self.dense3(combined_x)\n",
    "        return self.out_layer(combined_x)\n",
    "\n",
    "    # 3. metrics property\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "        # return a list with all metrics in the model\n",
    "\n",
    "    # 4. reset all metrics objects\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "            \n",
    "    def get_sample_weights(self, label):\n",
    "        \"\"\"\n",
    "        Tensor of shape (1,32), containing 0.9 where the label is 1 and 0.1 where the label is 0\n",
    "        \"\"\"\n",
    "        return tf.reshape(tf.where(tf.cast(label,tf.bool), 0.9, 0.1), (1,-1))\n",
    "\n",
    "    # 5. train step method\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        img1, img2, label = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self((img1, img2), training=True)\n",
    "            binary_sample_weights = self.get_sample_weights(label)\n",
    "            loss = self.loss_function(label, output, sample_weight=binary_sample_weights)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # update the state of the metrics according to loss\n",
    "        self.metrics[0].update_state(label, output)\n",
    "        self.metrics[1].update_state(loss)\n",
    "        \n",
    "        # return a dictionary with metric names as keys and metric results as values\n",
    "        return {m.name : m.result() for m in self.metrics}\n",
    "\n",
    "    # 6. test_step method\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        img1, img2, label = data\n",
    "        # same as train step (without parameter updates)\n",
    "        output = self((img1, img2), training=False)\n",
    "        binary_sample_weights = self.get_sample_weights(label)\n",
    "        loss = self.loss_function(label, output, sample_weight=binary_sample_weights)\n",
    "        self.metrics[0].update_state(label, output)\n",
    "        self.metrics[1].update_state(loss)\n",
    "        \n",
    "        return {m.name : m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:04<00:00, 390.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9121500253677368', 'loss: 0.04621931165456772']\n",
      "['val_binary_accuracy: 0.9301999807357788', 'val_loss: 0.03250011056661606']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:04<00:00, 466.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9449999928474426', 'loss: 0.02709951251745224']\n",
      "['val_binary_accuracy: 0.9544000029563904', 'val_loss: 0.023736122995615005']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:04<00:00, 463.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9616166949272156', 'loss: 0.021192438900470734']\n",
      "['val_binary_accuracy: 0.9593999981880188', 'val_loss: 0.02040999010205269']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:03<00:00, 470.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9663666486740112', 'loss: 0.01814686320722103']\n",
      "['val_binary_accuracy: 0.9699000120162964', 'val_loss: 0.017676211893558502']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:04<00:00, 468.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9714333415031433', 'loss: 0.015792982652783394']\n",
      "['val_binary_accuracy: 0.9753999710083008', 'val_loss: 0.014733675867319107']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:04<00:00, 466.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9740833044052124', 'loss: 0.01481231115758419']\n",
      "['val_binary_accuracy: 0.9757999777793884', 'val_loss: 0.013603908009827137']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:04<00:00, 461.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9753000140190125', 'loss: 0.0138473529368639']\n",
      "['val_binary_accuracy: 0.9776999950408936', 'val_loss: 0.012952522374689579']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:03<00:00, 471.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9769999980926514', 'loss: 0.013092205859720707']\n",
      "['val_binary_accuracy: 0.9812999963760376', 'val_loss: 0.011002237908542156']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:05<00:00, 332.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9783166646957397', 'loss: 0.011921226978302002']\n",
      "['val_binary_accuracy: 0.974399983882904', 'val_loss: 0.013268744572997093']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:04<00:00, 444.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary_accuracy: 0.9800166487693787', 'loss: 0.011332252994179726']\n",
      "['val_binary_accuracy: 0.9793000221252441', 'val_loss: 0.011616488918662071']\n"
     ]
    }
   ],
   "source": [
    "# create new summary_writers\n",
    "train_summary_writer, val_summary_writer = create_summary_writers(config_name=\"RUN1_with_sample_weights\")\n",
    "\n",
    "# 1. instantiate model\n",
    "model = TwinMNISTModelSW()\n",
    "\n",
    "# 2. choose a path to save the weights\n",
    "save_path = \"trained_model_RUN1_sample_weights\"\n",
    "\n",
    "# 2. pass arguments to training loop function\n",
    "\n",
    "training_loop(model=model,\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    start_epoch=0,\n",
    "    epochs=10,\n",
    "    train_summary_writer=train_summary_writer,\n",
    "    val_summary_writer=val_summary_writer,\n",
    "    save_path=save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "54ff86533a6a943eb33cb0954e5964c6e356fb8134919fff31cf4713965c9c7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
