{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras metrics\n",
    "\n",
    "Instead of appending loss values to a list or a numpy array, we usually rely on something more convenient: tf.keras.metrics objects.\n",
    "\n",
    "Keras comes with useful objects that support tracking a variable, such as the loss over an entire epoch, computing the average over all losses efficiently while allowing for graph mode train and test step functions/methods.\n",
    "\n",
    "\n",
    "Let's assume we have one **epoch** with a small dataset that fills only **4 batches**, so to compute the average loss for this epoch, we want to average over four loss values. \n",
    "\n",
    "We can do this with a tf.keras.metrics.Mean object, which has an **update_state** method, that takes a scalar value to take into account for the running average, a **result** method, to obtain the result, and a **reset_states** method that resets the metric (after an epoch and between the training and validation steps).\n",
    "\n",
    "To see what is going on we print the metric's result after each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss: 15.873384475708008\n",
      "Epoch 0: val_loss: 16.585254669189453 \n",
      "\n",
      "Epoch 1: loss: 16.326271057128906\n",
      "Epoch 1: val_loss: 17.245628356933594 \n",
      "\n",
      "Epoch 2: loss: 16.922609329223633\n",
      "Epoch 2: val_loss: 16.337326049804688 \n",
      "\n",
      "Epoch 3: loss: 16.261056900024414\n",
      "Epoch 3: val_loss: 16.288795471191406 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "loss_function = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# instantiate metric object (usually in the model's constructor, i.e. __init__, method)\n",
    "loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "# ITERATING OVER A NUMBER OF EPOCHS:\n",
    "for e in range(4):\n",
    "\n",
    "    # ITERATING OVER TRAINING DATA\n",
    "    for batch in range(1000):\n",
    "\n",
    "        model_output = tf.random.uniform(shape=(4,1))*10\n",
    "        target = tf.random.uniform(shape=(4,1))*10\n",
    "\n",
    "        loss = loss_function(target, model_output)\n",
    "\n",
    "        loss_metric.update_state(values=loss)\n",
    "\n",
    "    tf.print(f\"Epoch {e}: loss: {loss_metric.result()}\")\n",
    "\n",
    "    # RESETTING THE METRICS\n",
    "    loss_metric.reset_states()\n",
    "\n",
    "    # ITERATING OVER VALIDATION DATA\n",
    "    for batch in range(200):\n",
    "\n",
    "        model_output = tf.random.uniform(shape=(4,1))*10\n",
    "\n",
    "        target = tf.random.uniform(shape=(4,1))*10\n",
    "\n",
    "        val_loss = loss_function(target, model_output)\n",
    "\n",
    "        loss_metric.update_state(values=val_loss)\n",
    "\n",
    "    tf.print(f\"Epoch {e}: val_loss: {loss_metric.result()} \\n\")\n",
    "\n",
    "    # RESETTING THE METRIC BEFORE NEXT EPOCH\n",
    "    loss_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy metrics for binary classification\n",
    "As you see we can use tf.keras.metrics.Mean objects to compute running averages without using numpy or list appends. Keras comes with such metric objects for a number of different metrics that we might use to evaluate our model's performance, such as **BinaryAccuracy** in the context of binary classification and **CategoricalAccuracy** **TopKCategoricalAccuracy** in the context of multi-class classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss: 1.0046213865280151\n",
      "Epoch 0: accuracy: 0.5099999904632568\n",
      "Epoch 0: val_loss: 0.9262159466743469\n",
      "Epoch 0: val_accuracy: 0.5874999761581421 \n",
      "\n",
      "Epoch 1: loss: 0.9797967076301575\n",
      "Epoch 1: accuracy: 0.5229166746139526\n",
      "Epoch 1: val_loss: 0.7920961380004883\n",
      "Epoch 1: val_accuracy: 0.574999988079071 \n",
      "\n",
      "Epoch 2: loss: 1.0067111253738403\n",
      "Epoch 2: accuracy: 0.49166667461395264\n",
      "Epoch 2: val_loss: 1.25363290309906\n",
      "Epoch 2: val_accuracy: 0.4124999940395355 \n",
      "\n",
      "Epoch 3: loss: 1.0154286623001099\n",
      "Epoch 3: accuracy: 0.4937500059604645\n",
      "Epoch 3: val_loss: 0.8198896646499634\n",
      "Epoch 3: val_accuracy: 0.5375000238418579 \n",
      "\n",
      "Epoch 4: loss: 1.0363298654556274\n",
      "Epoch 4: accuracy: 0.49791666865348816\n",
      "Epoch 4: val_loss: 1.026277780532837\n",
      "Epoch 4: val_accuracy: 0.44999998807907104 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instantiate metric object (usually in the model's constructor, i.e. __init__, method)\n",
    "loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n",
    "binary_accuracy_metric = tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")\n",
    "loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# What happens in one epoch in the training loop:\n",
    "\n",
    "# training on train data (4 batches)\n",
    "\n",
    "for e in range(5):\n",
    "    for train_batch in range(100):\n",
    "        target = tf.random.uniform(shape=(4,1),minval=0, maxval=1, dtype=tf.int32)\n",
    "        model_output = tf.random.uniform(shape=(4,1))\n",
    "\n",
    "        loss = loss_function(target, model_output)\n",
    "        # \n",
    "        loss_metric.update_state(values=loss)\n",
    "        binary_accuracy_metric.update_state(target, model_output)\n",
    "    tf.print(f\"Epoch {e}: loss: {loss_metric.result()}\")\n",
    "    tf.print(f\"Epoch {e}: accuracy: {binary_accuracy_metric.result()}\")\n",
    "\n",
    "    # RESETTING METRICS BEFORE EVALUATION\n",
    "    loss_metric.reset_states()\n",
    "    binary_accuracy_metric.reset_states()\n",
    "\n",
    "    for val_batch in range(20):\n",
    "        target = tf.random.uniform(shape=(4,1),minval=0, maxval=1, dtype=tf.int32)\n",
    "        model_output = tf.random.uniform(shape=(4,1))\n",
    "\n",
    "        loss = loss_function(target, model_output)\n",
    "        loss_metric.update_state(values=loss)\n",
    "        binary_accuracy_metric.update_state(target, model_output)\n",
    "\n",
    "    tf.print(f\"Epoch {e}: val_loss: {loss_metric.result()}\")\n",
    "    tf.print(f\"Epoch {e}: val_accuracy: {binary_accuracy_metric.result()} \\n\")\n",
    "\n",
    "# resetting the metric before the next epoch\n",
    "loss_metric.reset_states()\n",
    "binary_accuracy_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy metrics for multi-class categorization tasks\n",
    "\n",
    "In multi-class classification, we use the **CategoricalCrossentropy** as our loss function. To track the accuracy, we need to use a different keras metric, **CategoricalAccuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss: 2.3252077102661133\n",
      "Epoch 0: accuracy: 0.10000000149011612\n",
      "Epoch 0: val_loss: 2.3609747886657715\n",
      "Epoch 0: val_accuracy: 0.125 \n",
      "\n",
      "Epoch 1: loss: 2.3456506729125977\n",
      "Epoch 1: accuracy: 0.09749999642372131\n",
      "Epoch 1: val_loss: 2.370378255844116\n",
      "Epoch 1: val_accuracy: 0.0625 \n",
      "\n",
      "Epoch 2: loss: 2.3308346271514893\n",
      "Epoch 2: accuracy: 0.10750000178813934\n",
      "Epoch 2: val_loss: 2.3247058391571045\n",
      "Epoch 2: val_accuracy: 0.15000000596046448 \n",
      "\n",
      "Epoch 3: loss: 2.308459997177124\n",
      "Epoch 3: accuracy: 0.11249999701976776\n",
      "Epoch 3: val_loss: 2.3280162811279297\n",
      "Epoch 3: val_accuracy: 0.11249999701976776 \n",
      "\n",
      "Epoch 4: loss: 2.346121311187744\n",
      "Epoch 4: accuracy: 0.09749999642372131\n",
      "Epoch 4: val_loss: 2.358342170715332\n",
      "Epoch 4: val_accuracy: 0.0625 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instantiate metric object (usually in the model's constructor, i.e. __init__, method)\n",
    "loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n",
    "accuracy_metric = tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "for e in range(5):\n",
    "    for train_batch in range(100):\n",
    "        \n",
    "        # create random one-hot targets (labels)\n",
    "        target = tf.one_hot(tf.random.uniform(minval=0,maxval=9, shape=(4,),dtype=tf.int32), depth=10)\n",
    "\n",
    "        # create random model output (for each element in the batch the values are probabilities that sum to 1)\n",
    "        model_output = tf.nn.softmax(tf.random.uniform(shape=(4,10)), axis=-1)\n",
    "\n",
    "        loss = loss_function(target, model_output)\n",
    "\n",
    "        loss_metric.update_state(values=loss)\n",
    "        accuracy_metric.update_state(target, model_output)\n",
    "    tf.print(f\"Epoch {e}: loss: {loss_metric.result()}\")\n",
    "    tf.print(f\"Epoch {e}: accuracy: {accuracy_metric.result()}\")\n",
    "\n",
    "    # RESETTING METRICS BEFORE EVALUATION\n",
    "    loss_metric.reset_states()\n",
    "    accuracy_metric.reset_states()\n",
    "\n",
    "    for val_batch in range(20):\n",
    "        target = tf.one_hot(tf.random.uniform(minval=0,maxval=9, shape=(4,),dtype=tf.int32), depth=10)\n",
    "        model_output = tf.nn.softmax(tf.random.uniform(shape=(4,10)), axis=-1)\n",
    "\n",
    "        loss = loss_function(target, model_output)\n",
    "        loss_metric.update_state(values=loss)\n",
    "        accuracy_metric.update_state(target, model_output)\n",
    "\n",
    "    tf.print(f\"Epoch {e}: val_loss: {loss_metric.result()}\")\n",
    "    tf.print(f\"Epoch {e}: val_accuracy: {accuracy_metric.result()} \\n\")\n",
    "\n",
    "    # resetting the metric before the next epoch\n",
    "    loss_metric.reset_states()\n",
    "    accuracy_metric.reset_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- We can use keras metrics instead of tedious numpy loss tracking and metric computations. \n",
    "\n",
    "- Keras metrics can be used with Tensorflow's graph mode, while list appends and numpy operations generally can not\n",
    "\n",
    "- The convenient compile and fit methods of the tf.keras.Model class use keras metrics under the hood. \n",
    "    - Starting to use the metric objects gets us a step closer to being able to use these tools.\n",
    "\n",
    "\n",
    "\n",
    "Next: **Tensorboard for logging**, log all possible kinds of training data to the tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54ff86533a6a943eb33cb0954e5964c6e356fb8134919fff31cf4713965c9c7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
